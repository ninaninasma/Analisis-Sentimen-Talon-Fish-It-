{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bd91c53c",
      "metadata": {
        "id": "bd91c53c"
      },
      "source": [
        "**Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4acdbcb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "4acdbcb8",
        "outputId": "783b5f44-192a-4214-981f-311b3b13b1ab"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import emoji\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174be1c1",
      "metadata": {
        "id": "174be1c1"
      },
      "source": [
        "<h1>Baca Data<h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "066498b3",
      "metadata": {
        "id": "066498b3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data.csv\")\n",
        "texts = df[\"full_text\"].astype(str)\n",
        "texts.to_csv(\"data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db2f14c",
      "metadata": {
        "id": "4db2f14c"
      },
      "source": [
        "<h1>PraPemrosesan import Stemmer dari Sastrawi</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "069f6f0f",
      "metadata": {
        "id": "069f6f0f"
      },
      "outputs": [],
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "stop_nltk = set(stopwords.words('indonesian'))\n",
        "stop_sastrawi = set(StopWordRemoverFactory().get_stop_words())\n",
        "stop_words = stop_nltk.union(stop_sastrawi)\n",
        "\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xTguowq-8eHk",
      "metadata": {
        "id": "xTguowq-8eHk"
      },
      "source": [
        "Stemmer dari Sastrawi untuk mereduksi kata-kata Indonesia ke bentuk dasar. Kode menggabungkan daftar stopwords dari NLTK dan Sastrawi ke dalam satu himpunan stop_words untuk memfilter kata-kata umum. Selain itu, RegexpTokenizer dengan pola r'\\w+' digunakan untuk memisahkan teks menjadi token alfanumerik, menghilangkan tanda baca. Pengaturan ini mendukung pra-pemrosesan teks untuk analisis seperti sentimen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83cc2f12",
      "metadata": {
        "id": "83cc2f12"
      },
      "source": [
        "<h1>Pembersihan data menggunakan case folding CPMK 2<h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ebe69659",
      "metadata": {
        "id": "ebe69659"
      },
      "outputs": [],
      "source": [
        "def case_folding(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "df_new = pd.DataFrame({\n",
        "    \"full_text\": texts,\n",
        "    \"case_folding\": texts.apply(case_folding)\n",
        "})\n",
        "\n",
        "df_new.to_csv(\"data_casefolding.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i50PS7gH84OK",
      "metadata": {
        "id": "i50PS7gH84OK"
      },
      "source": [
        "Mengubah teks menjadi huruf kecil untuk standardisasi. Fungsi tersebut diterapkan pada daftar teks dalam variabel texts untuk membuat kolom baru bernama case_folding. Kode kemudian membuat DataFrame baru df_new menggunakan pandas, yang berisi kolom full_text (teks asli) dan case_folding (teks setelah konversi huruf kecil). Terakhir, DataFrame ini disimpan ke file CSV bernama data_casefolding.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pwuhi4B21IaX",
      "metadata": {
        "id": "Pwuhi4B21IaX"
      },
      "source": [
        "<h1>Preprocessing regex, tokenisasi dan pemakaian stemmer CPMK 3<h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1apKJQF209qo",
      "metadata": {
        "id": "1apKJQF209qo"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
        "    text = re.sub(r'@\\w+', ' ', text)\n",
        "    text = emoji.demojize(text)\n",
        "    text = re.sub(r'\\d+',' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "    text = re.sub(r'\\s+',' ', text).strip()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df[\"preprocessing\"] = texts.apply(clean_text)\n",
        "\n",
        "df_new = pd.DataFrame({\n",
        "    \"full_text\": texts,\n",
        "    \"case_folding CPMK2\": texts.apply(case_folding),\n",
        "    \"preprocessing CPMK3\": df[\"preprocessing\"]\n",
        "})\n",
        "\n",
        "df_new.to_csv(\"data_prepocessing.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97c78113",
      "metadata": {},
      "source": [
        "Preprocessing teks dengan fungsi clean_text untuk membersihkan dan menstandarisasi data teks agar lebih mudah dianalisis. Proses ini melibatkan beberapa langkah: mengubah teks menjadi huruf kecil (case folding), menghapus URL (dengan pola http\\S+|www\\S+), menghapus mention seperti @username, mengonversi emoji menjadi representasi teks menggunakan emoji.demojize, menghapus angka, menghapus tanda baca dan karakter khusus, mengurangi karakter berulang (misalnya \"aaa\" menjadi \"a\"), serta menghapus spasi berlebih untuk menghasilkan teks yang bersih dan terstandar. Hasilnya adalah teks yang lebih konsisten dan siap untuk tahap analisis berikutnya, seperti tokenisasi."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c69a37c",
      "metadata": {},
      "source": [
        "Tokenisasi dengan memanggil tokenizer.tokenize(text), yang memecah teks yang telah dibersihkan menjadi unit-unit kecil yang disebut token (biasanya kata-kata individual). Proses ini penting karena memungkinkan analisis teks pada tingkat kata, bukan sebagai satu string panjang. Tokenisasi memisahkan teks berdasarkan spasi atau aturan tertentu yang ditentukan oleh tokenizer yang digunakan (misalnya, library seperti NLTK atau spaCy). Tokenisasi menghasilkan daftar kata-kata yang kemudian akan diproses lebih lanjut, seperti menghapus stop words dan melakukan stemming untuk analisis teks yang lebih mendalam."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a1a9d85",
      "metadata": {},
      "source": [
        "Stemming dilakukan dengan menggunakan stemmer.stem(w) pada setiap token setelah tokenisasi dan penghapusan stop words. Stemming adalah proses mengurangi kata ke bentuk dasarnya atau akarnya (stem) dengan menghapus imbuhan, seperti mengubah \"berlari\" menjadi \"lari\" atau \"running\" menjadi \"run\". Tujuannya adalah untuk menyatukan variasi kata yang memiliki makna serupa agar analisis teks lebih efisien, misalnya dalam text mining atau klasifikasi teks. Stemming diterapkan pada setiap token untuk menghasilkan daftar kata-kata dalam bentuk dasar yang kemudian digabung kembali menjadi string dengan spasi sebagai pemisah untuk disimpan dalam kolom \"preprocessing\"."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
