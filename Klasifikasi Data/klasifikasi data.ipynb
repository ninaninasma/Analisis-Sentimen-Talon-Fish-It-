{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07601ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f083359c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>case_folding CPMK2</th>\n",
       "      <th>preprocessing CPMK3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yeay naik secret elshark gran maja 1/4m pulau ...</td>\n",
       "      <td>yeay naik secret elshark gran maja 1/4m pulau ...</td>\n",
       "      <td>yeay secret elshark gran maja pulau pulang mar...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>semenjak kenal fish aku melupakan gunung</td>\n",
       "      <td>semenjak kenal fish aku melupakan gunung</td>\n",
       "      <td>semenjak kenal lupa gunung</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baru banget aku main fish semalam langsung keb...</td>\n",
       "      <td>baru banget aku main fish semalam langsung keb...</td>\n",
       "      <td>main malam langsung bawa mimpi</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mending mancing fish</td>\n",
       "      <td>mending mancing fish</td>\n",
       "      <td>pancing</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dunia ini sudah gila kayanya ditiktok gue nyar...</td>\n",
       "      <td>dunia ini sudah gila kayanya ditiktok gue nyar...</td>\n",
       "      <td>dunia gila kaya ditiktok cari kraken mythology...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>pengen nyobain ikutan fish tapi takut akun aku...</td>\n",
       "      <td>pengen nyobain ikutan fish tapi takut akun aku...</td>\n",
       "      <td>ken nyobain ikut takut akun bau level</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>butuh banget teman main fish bingung kalau dap...</td>\n",
       "      <td>butuh banget teman main fish bingung kalau dap...</td>\n",
       "      <td>butuh teman main bingung secret pamer</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>dari fish kita belajar semua punya proses masi...</td>\n",
       "      <td>dari fish kita belajar semua punya proses masi...</td>\n",
       "      <td>ajar proses left right arrow</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>kangen banget fish yang update tiap jam subuh ...</td>\n",
       "      <td>kangen banget fish yang update tiap jam subuh ...</td>\n",
       "      <td>kangen update jam subuh huwaa</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>gara2 fish tidak keurus ini map untung tinggal...</td>\n",
       "      <td>gara2 fish tidak keurus ini map untung tinggal...</td>\n",
       "      <td>urus map untung tinggal fitur fuck racun fishi...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             full_text  \\\n",
       "0    yeay naik secret elshark gran maja 1/4m pulau ...   \n",
       "1             semenjak kenal fish aku melupakan gunung   \n",
       "2    baru banget aku main fish semalam langsung keb...   \n",
       "3                                 mending mancing fish   \n",
       "4    dunia ini sudah gila kayanya ditiktok gue nyar...   \n",
       "..                                                 ...   \n",
       "245  pengen nyobain ikutan fish tapi takut akun aku...   \n",
       "246  butuh banget teman main fish bingung kalau dap...   \n",
       "247  dari fish kita belajar semua punya proses masi...   \n",
       "248  kangen banget fish yang update tiap jam subuh ...   \n",
       "249  gara2 fish tidak keurus ini map untung tinggal...   \n",
       "\n",
       "                                    case_folding CPMK2  \\\n",
       "0    yeay naik secret elshark gran maja 1/4m pulau ...   \n",
       "1             semenjak kenal fish aku melupakan gunung   \n",
       "2    baru banget aku main fish semalam langsung keb...   \n",
       "3                                 mending mancing fish   \n",
       "4    dunia ini sudah gila kayanya ditiktok gue nyar...   \n",
       "..                                                 ...   \n",
       "245  pengen nyobain ikutan fish tapi takut akun aku...   \n",
       "246  butuh banget teman main fish bingung kalau dap...   \n",
       "247  dari fish kita belajar semua punya proses masi...   \n",
       "248  kangen banget fish yang update tiap jam subuh ...   \n",
       "249  gara2 fish tidak keurus ini map untung tinggal...   \n",
       "\n",
       "                                   preprocessing CPMK3     label  \n",
       "0    yeay secret elshark gran maja pulau pulang mar...  positive  \n",
       "1                           semenjak kenal lupa gunung  negative  \n",
       "2                       main malam langsung bawa mimpi  negative  \n",
       "3                                              pancing  positive  \n",
       "4    dunia gila kaya ditiktok cari kraken mythology...  negative  \n",
       "..                                                 ...       ...  \n",
       "245              ken nyobain ikut takut akun bau level   neutral  \n",
       "246              butuh teman main bingung secret pamer   neutral  \n",
       "247                       ajar proses left right arrow   neutral  \n",
       "248                      kangen update jam subuh huwaa   neutral  \n",
       "249  urus map untung tinggal fitur fuck racun fishi...   neutral  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_250.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce44cb4",
   "metadata": {},
   "source": [
    "<h>PREPOCESSING</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd72f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "stop_words = set(stopwords.words(\"indonesian\"))\n",
    "\n",
    "stopwords_extra = {\n",
    "    'aku', 'nya', 'oke', 'aja', 'saya', 'sih', 'doang', 'gw', 'gua', 'aq', \n",
    "    'kamu', 'lu', 'loe', 'km', 'kmu', 'iya', 'ya', 'yah', 'nih', 'dong', \n",
    "    'lah', 'kah', 'deh', 'ajah', 'aj', 'gak', 'ga', 'gk', 'ngga', 'tp', \n",
    "    'tpi', 'sm', 'sma', 'lg', 'lgi', 'udh', 'udah', 'dah', 'blm', 'blom',\n",
    "    'klo', 'kalo', 'kl', 'gt', 'gtu', 'gini', 'gmn', 'gimana', 'wkwk', \n",
    "    'haha', 'hehe', 'huhu', 'anjir', 'anjing', 'bjir', 'cok', 'coy', \n",
    "    'dm', 'btw', 'pls', 'plis', 'brp', 'brpa', 'kak', 'bang', 'bro', \n",
    "    'sis', 'pun', 'mah', 'tu', 'nih', 'https', 'http', 'www', 'co', \n",
    "    'id', 'net', 'org', 'yuk', 'ken', \"gue\", \"guys\", \"tau\", \"tuh\", \"daki\",\n",
    "    \"biar\", \"kali\", \"banget\", 'mending', 'gitu', \"kakak\", \"pas\", \"karena\",\n",
    "    \"ayo\", \"semoga\", \"kena\", \"ajaa\", \"gara gara\", \"sender\", \"gara\", \"pake\", \n",
    "    \"emang\", \"txtfromroblox\", \"by the way\", \"karna\", \"avqris avrekber okii\"\n",
    "}\n",
    "\n",
    "stop_words.update(stopwords_extra)\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8ef290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"koreksi.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    abbreviation_dict = json.load(f)\n",
    "\n",
    "bentuk_kata = re.compile(r'^[a-z]+$')\n",
    "\n",
    "def reduce_repeated_chars(word):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
    "\n",
    "def remove_non_latin(text):\n",
    "    return ''.join(\n",
    "        c for c in text \n",
    "        if unicodedata.category(c)[0] != 'C'\n",
    "        and \"LATIN\" in unicodedata.name(c, 'LATIN')\n",
    "    )\n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "def remove_non_latin(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', ' ', text)  \n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
    "    text = emoji.replace_emoji(text, replace=' ')\n",
    "    text = re.sub(r'[@#]\\w+', ' ', text)\n",
    "\n",
    "    text = remove_non_latin(text)\n",
    "\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    words = text.split()\n",
    "    cleaned = []\n",
    "\n",
    "    for w in words:\n",
    "        w = reduce_repeated_chars(w)\n",
    "\n",
    "        if w in abbreviation_dict:\n",
    "            w = abbreviation_dict[w]\n",
    "\n",
    "        if not bentuk_kata.match(w):\n",
    "            continue\n",
    "\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "\n",
    "        w = stemmer.stem(w)\n",
    "\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "\n",
    "        cleaned.append(w)\n",
    "\n",
    "    cleaned_text = \" \".join(cleaned)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cfbb8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      yeay secret elshark gran maja pulau pulang mar...\n",
       "1                             semenjak kenal lupa gunung\n",
       "2                         main malam langsung bawa mimpi\n",
       "3                                                pancing\n",
       "4      dunia gila kaya ditiktok cari kraken mythology...\n",
       "                             ...                        \n",
       "245                         nyobain takut akun bau level\n",
       "246                butuh teman main bingung secret pamer\n",
       "247                         ajar proses left right arrow\n",
       "248                        kangen update jam subuh huwaa\n",
       "249    urus map untung tinggal fitur fuck racun fishi...\n",
       "Name: bersih, Length: 250, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"bersih\"] = df[\"preprocessing CPMK3\"].apply(preprocess)\n",
    "df['bersih']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350cbc2",
   "metadata": {},
   "source": [
    "<h>EKSPERIMEN KLASIFIKASI</h>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11264f2c",
   "metadata": {},
   "source": [
    "<h>SVM</h>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8081a",
   "metadata": {},
   "source": [
    "1. SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71f511df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 0.54\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.63      0.60        19\n",
      "     neutral       0.20      0.25      0.22         8\n",
      "    positive       0.68      0.57      0.62        23\n",
      "\n",
      "    accuracy                           0.54        50\n",
      "   macro avg       0.49      0.48      0.48        50\n",
      "weighted avg       0.56      0.54      0.55        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ============================\n",
    "# 1. LOAD DATASET\n",
    "# ============================\n",
    "\n",
    "# Pisahkan fitur dan label\n",
    "X = df['bersih']\n",
    "y = df['label']\n",
    "\n",
    "# ============================\n",
    "# 2. SPLIT DATA TRAIN & TEST\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. TF-IDF FEATURE EXTRACTION\n",
    "# ============================\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,      \n",
    "    ngram_range=(1,2),      \n",
    "    stop_words=None         \n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "# ============================\n",
    "# 4. TRAIN MODEL SVM\n",
    "# ============================\n",
    "model = LinearSVC()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# ============================\n",
    "# 5. PREDIKSI\n",
    "# ============================\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# ============================\n",
    "# 6. EVALUASI\n",
    "# ============================\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Akurasi:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423afbe",
   "metadata": {},
   "source": [
    "2. SVM + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9f72a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Akurasi BoW + SVM: 0.62\n",
      "=======================================\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.65      0.63        20\n",
      "     neutral       1.00      0.10      0.18        10\n",
      "    positive       0.61      0.85      0.71        20\n",
      "\n",
      "    accuracy                           0.62        50\n",
      "   macro avg       0.74      0.53      0.51        50\n",
      "weighted avg       0.69      0.62      0.57        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ============================\n",
    "# 1. LOAD DATASET\n",
    "# ============================\n",
    "\n",
    "X = df['bersih']     # fitur teks\n",
    "y = df['label']      # label sentimen\n",
    "\n",
    "# ============================\n",
    "# 2. SPLIT DATA\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. EKSTRAKSI FITUR: BAG OF WORDS\n",
    "# ============================\n",
    "bow = CountVectorizer(\n",
    "    max_features=5000,     # batasi jumlah fitur\n",
    "    ngram_range=(1,2),     # unigram + bigram\n",
    "    stop_words=None\n",
    ")\n",
    "\n",
    "X_train_bow = bow.fit_transform(X_train)\n",
    "X_test_bow  = bow.transform(X_test)\n",
    "\n",
    "# ============================\n",
    "# 4. MODEL SVM\n",
    "# ============================\n",
    "model = LinearSVC()\n",
    "model.fit(X_train_bow, y_train)\n",
    "\n",
    "# ============================\n",
    "# 5. PREDIKSI\n",
    "# ============================\n",
    "y_pred = model.predict(X_test_bow)\n",
    "\n",
    "# ============================\n",
    "# 6. EVALUASI\n",
    "# ============================\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(\"Akurasi BoW + SVM:\", accuracy)\n",
    "print(\"=======================================\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379c11a",
   "metadata": {},
   "source": [
    "<h>LOGISTIC REGRESION</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0112f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ninak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\ninak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.13.0)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ninak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB 660.6 kB/s eta 0:00:37\n",
      "   ---------------------------------------- 0.0/24.4 MB 495.5 kB/s eta 0:00:50\n",
      "   ---------------------------------------- 0.1/24.4 MB 491.5 kB/s eta 0:00:50\n",
      "   ---------------------------------------- 0.1/24.4 MB 722.1 kB/s eta 0:00:34\n",
      "   ---------------------------------------- 0.2/24.4 MB 1.0 MB/s eta 0:00:24\n",
      "   ---------------------------------------- 0.2/24.4 MB 1.0 MB/s eta 0:00:24\n",
      "   ---------------------------------------- 0.2/24.4 MB 1.0 MB/s eta 0:00:24\n",
      "    --------------------------------------- 0.3/24.4 MB 863.3 kB/s eta 0:00:28\n",
      "    --------------------------------------- 0.5/24.4 MB 1.3 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.6/24.4 MB 1.4 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 0.7/24.4 MB 1.3 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.7/24.4 MB 1.3 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.7/24.4 MB 1.3 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.7/24.4 MB 1.3 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.7/24.4 MB 1.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 1.0/24.4 MB 1.3 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 1.1/24.4 MB 1.5 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 1.3/24.4 MB 1.6 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 1.5/24.4 MB 1.7 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.8/24.4 MB 1.9 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.4 MB 2.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 2.3/24.4 MB 2.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 2.3/24.4 MB 2.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 2.4/24.4 MB 2.2 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.7/24.4 MB 2.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 2.5 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 2.5 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 2.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 2.1 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 2.2 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 2.2 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 3.3/24.4 MB 2.1 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 3.5/24.4 MB 2.2 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 3.9/24.4 MB 2.3 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 4.4/24.4 MB 2.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 4.9/24.4 MB 2.8 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.2/24.4 MB 2.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 5.9/24.4 MB 3.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.0/24.4 MB 3.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.0/24.4 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 3.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.7/24.4 MB 3.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.2/24.4 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.4/24.4 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.8/24.4 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.3/24.4 MB 3.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 8.7/24.4 MB 3.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/24.4 MB 4.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/24.4 MB 4.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/24.4 MB 4.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/24.4 MB 4.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/24.4 MB 4.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/24.4 MB 4.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.9/24.4 MB 4.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 4.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 10.9/24.4 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.0/24.4 MB 5.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.2/24.4 MB 5.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.6/24.4 MB 5.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.3/24.4 MB 5.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.3/24.4 MB 5.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.3/24.4 MB 5.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.8/24.4 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.8/24.4 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.8/24.4 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.8/24.4 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.9/24.4 MB 5.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.9/24.4 MB 5.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.9/24.4 MB 5.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.9/24.4 MB 4.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.2/24.4 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.2/24.4 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.2/24.4 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.2/24.4 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.2/24.4 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.3/24.4 MB 4.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.3/24.4 MB 4.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.3/24.4 MB 4.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.3/24.4 MB 4.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.3/24.4 MB 4.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.3/24.4 MB 4.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.4/24.4 MB 4.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.4/24.4 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.4/24.4 MB 4.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 4.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.9/24.4 MB 4.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 4.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.9/24.4 MB 4.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.2/24.4 MB 4.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.4/24.4 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.4/24.4 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.5/24.4 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.5/24.4 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.5/24.4 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.6/24.4 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.8/24.4 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 16.2/24.4 MB 2.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.5/24.4 MB 2.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.6/24.4 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.4/24.4 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 17.9/24.4 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.4/24.4 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.7/24.4 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.4 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.6/24.4 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.4/24.4 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.5/24.4 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.5/24.4 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.7/24.4 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.4 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.1/24.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.7/24.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.7/24.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.8/24.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.4 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.9/63.9 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: smart_open, gensim\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea669d",
   "metadata": {},
   "source": [
    "1. LOGISTIC REGRESION + WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebf091e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Akurasi Word2Vec + Logistic Regression: 0.46\n",
      "=======================================\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.55      0.49        20\n",
      "     neutral       0.00      0.00      0.00        10\n",
      "    positive       0.48      0.60      0.53        20\n",
      "\n",
      "    accuracy                           0.46        50\n",
      "   macro avg       0.31      0.38      0.34        50\n",
      "weighted avg       0.37      0.46      0.41        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# ============================\n",
    "# 1. LOAD DATASET\n",
    "# ============================\n",
    "X = df['bersih']\n",
    "y = df['label']\n",
    "\n",
    "# Tokenisasi sederhana (Word2Vec butuh token list)\n",
    "tokenized = X.apply(lambda x: x.split())\n",
    "\n",
    "# ============================\n",
    "# 2. SPLIT DATA\n",
    "# ============================\n",
    "X_train_tokens, X_test_tokens, y_train, y_test = train_test_split(\n",
    "    tokenized, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. TRAIN WORD2VEC\n",
    "# ============================\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=X_train_tokens,\n",
    "    vector_size=100,   # dimensi vektor\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 4. FUNGSI RATA-RATA VEKTOR WORD2VEC\n",
    "# ============================\n",
    "def document_vector(tokens):\n",
    "    \"\"\"Rata-rata vektor Word2Vec dari semua kata dalam 1 dokumen.\"\"\"\n",
    "    vectors = [\n",
    "        w2v_model.wv[word] \n",
    "        for word in tokens \n",
    "        if word in w2v_model.wv\n",
    "    ]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(w2v_model.vector_size)   # jika tidak ada kata yang dikenal\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Convert semua dokumen ke vektor numeric\n",
    "X_train_vec = np.array([document_vector(tokens) for tokens in X_train_tokens])\n",
    "X_test_vec  = np.array([document_vector(tokens) for tokens in X_test_tokens])\n",
    "\n",
    "# ============================\n",
    "# 5. MODEL LOGISTIC REGRESSION\n",
    "# ============================\n",
    "logreg = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    solver='lbfgs',\n",
    "    multi_class='auto'\n",
    ")\n",
    "\n",
    "logreg.fit(X_train_vec, y_train)\n",
    "\n",
    "# ============================\n",
    "# 6. PREDIKSI\n",
    "# ============================\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "# ============================\n",
    "# 7. EVALUASI\n",
    "# ============================\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(\"Akurasi Word2Vec + Logistic Regression:\", accuracy)\n",
    "print(\"=======================================\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413d54c",
   "metadata": {},
   "source": [
    "2. LOGISTIC REGRESION + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf31834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Akurasi TF-IDF + Logistic Regression: 0.64\n",
      "=======================================\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.80      0.68        20\n",
      "     neutral       0.00      0.00      0.00        10\n",
      "    positive       0.70      0.80      0.74        20\n",
      "\n",
      "    accuracy                           0.64        50\n",
      "   macro avg       0.43      0.53      0.48        50\n",
      "weighted avg       0.52      0.64      0.57        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ============================\n",
    "# 1. LOAD DATASET\n",
    "# ============================\n",
    "X = df['bersih']\n",
    "y = df['label']\n",
    "\n",
    "# ============================\n",
    "# 2. SPLIT DATA\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. TF-IDF FEATURE EXTRACTION\n",
    "# ============================\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1,2),     # unigram + bigram\n",
    "    sublinear_tf=True,     # tf smoothing\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "# ============================\n",
    "# 4. MODEL LOGISTIC REGRESSION\n",
    "# ============================\n",
    "logreg = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    solver='lbfgs',\n",
    "    multi_class='auto'\n",
    ")\n",
    "\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# ============================\n",
    "# 5. PREDIKSI\n",
    "# ============================\n",
    "y_pred = logreg.predict(X_test_tfidf)\n",
    "\n",
    "# ============================\n",
    "# 6. EVALUASI\n",
    "# ============================\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(\"Akurasi TF-IDF + Logistic Regression:\", accuracy)\n",
    "print(\"=======================================\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea31ea",
   "metadata": {},
   "source": [
    "<h>NAIVE BAYES</h>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93180b53",
   "metadata": {},
   "source": [
    "1. NAIVE BAYES + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7dc9fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Akurasi BoW + Naive Bayes: 0.58\n",
      "=======================================\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        20\n",
      "     neutral       0.33      0.10      0.15        10\n",
      "    positive       0.65      0.65      0.65        20\n",
      "\n",
      "    accuracy                           0.58        50\n",
      "   macro avg       0.51      0.50      0.48        50\n",
      "weighted avg       0.55      0.58      0.55        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ============================\n",
    "# 1. LOAD DATASET\n",
    "# ============================\n",
    "X = df['bersih']\n",
    "y = df['label']\n",
    "\n",
    "# ============================\n",
    "# 2. SPLIT DATA\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. BAG OF WORDS (BoW)\n",
    "# ============================\n",
    "bow = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1,2),   # unigram + bigram\n",
    "    stop_words=None\n",
    ")\n",
    "\n",
    "X_train_bow = bow.fit_transform(X_train)\n",
    "X_test_bow  = bow.transform(X_test)\n",
    "\n",
    "# ============================\n",
    "# 4. MODEL NAIVE BAYES\n",
    "# ============================\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_bow, y_train)\n",
    "\n",
    "# ============================\n",
    "# 5. PREDIKSI\n",
    "# ============================\n",
    "y_pred = nb_model.predict(X_test_bow)\n",
    "\n",
    "# ============================\n",
    "# 6. EVALUASI\n",
    "# ============================\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(\"Akurasi BoW + Naive Bayes:\", accuracy)\n",
    "print(\"=======================================\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969c6e9",
   "metadata": {},
   "source": [
    "2. NAIVE BAYES + WOR2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "994c2385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Akurasi Word2Vec + Naive Bayes: 0.42\n",
      "=======================================\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.30      0.34        20\n",
      "     neutral       0.20      0.30      0.24        10\n",
      "    positive       0.60      0.60      0.60        20\n",
      "\n",
      "    accuracy                           0.42        50\n",
      "   macro avg       0.40      0.40      0.39        50\n",
      "weighted avg       0.44      0.42      0.43        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# ============================\n",
    "# 1. LOAD DATASET\n",
    "# ============================\n",
    "X = df['bersih']\n",
    "y = df['label']\n",
    "\n",
    "# Tokenisasi sederhana (untuk Word2Vec)\n",
    "sentences = [text.split() for text in X]\n",
    "\n",
    "# ============================\n",
    "# 2. TRAIN WORD2VEC MODEL\n",
    "# ============================\n",
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. FUNGSI RATA-RATA VEKTOR\n",
    "# ============================\n",
    "def vectorize_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    vectors = [\n",
    "        w2v_model.wv[word] for word in words if word in w2v_model.wv\n",
    "    ]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(100)   # Jika kalimat tidak punya kata dalam vocab\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Konversi seluruh dataset ke vektor\n",
    "X_vectors = np.array([vectorize_sentence(text) for text in X])\n",
    "\n",
    "# ============================\n",
    "# 4. SPLIT DATA\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vectors, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 5. MODEL NAIVE BAYES (GaussianNB)\n",
    "# ============================\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# ============================\n",
    "# 6. PREDIKSI & EVALUASI\n",
    "# ============================\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(\"Akurasi Word2Vec + Naive Bayes:\", accuracy)\n",
    "print(\"=======================================\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99059b7",
   "metadata": {},
   "source": [
    "<h>RANDOM FOREST</h>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2068d90",
   "metadata": {},
   "source": [
    "1. RANDOM FOREST + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6f67416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Akurasi TF-IDF + Random Forest: 0.56\n",
      "=======================================\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.50      0.56        20\n",
      "     neutral       0.00      0.00      0.00        10\n",
      "    positive       0.55      0.90      0.68        20\n",
      "\n",
      "    accuracy                           0.56        50\n",
      "   macro avg       0.39      0.47      0.41        50\n",
      "weighted avg       0.47      0.56      0.49        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ============================\n",
    "# 1. LOAD DATASET\n",
    "# ============================\n",
    "X = df['bersih']\n",
    "y = df['label']\n",
    "\n",
    "# ============================\n",
    "# 2. SPLIT DATA\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. TF-IDF FEATURE EXTRACTION\n",
    "# ============================\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),   # unigram + bigram\n",
    "    stop_words=None\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "# ============================\n",
    "# 4. RANDOM FOREST MODEL\n",
    "# ============================\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,        # jumlah pohon\n",
    "    max_depth=None,         # bebas, biarkan model cari sendiri\n",
    "    random_state=42,\n",
    "    n_jobs=-1               # pakai semua CPU\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# ============================\n",
    "# 5. PREDIKSI\n",
    "# ============================\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# ============================\n",
    "# 6. EVALUASI\n",
    "# ============================\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(\"Akurasi TF-IDF + Random Forest:\", accuracy)\n",
    "print(\"=======================================\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e09e47",
   "metadata": {},
   "source": [
    "2. RANDOM FOREST + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "498eee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Akurasi BoW + Random Forest: 0.5\n",
      "=======================================\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.25      0.34        20\n",
      "     neutral       0.00      0.00      0.00        10\n",
      "    positive       0.49      1.00      0.66        20\n",
      "\n",
      "    accuracy                           0.50        50\n",
      "   macro avg       0.35      0.42      0.33        50\n",
      "weighted avg       0.42      0.50      0.40        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ============================\n",
    "# 1. LOAD DATASET\n",
    "# ============================\n",
    "X = df['bersih']\n",
    "y = df['label']\n",
    "\n",
    "# ============================\n",
    "# 2. TRAIN TEST SPLIT\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. FEATURE EXTRACTION: BOW\n",
    "# ============================\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),    # unigram + bigram\n",
    "    stop_words=None\n",
    ")\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow  = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# ============================\n",
    "# 4. RANDOM FOREST MODEL\n",
    "# ============================\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,   \n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_bow, y_train)\n",
    "\n",
    "# ============================\n",
    "# 5. PREDIKSI\n",
    "# ============================\n",
    "y_pred = rf_model.predict(X_test_bow)\n",
    "\n",
    "# ============================\n",
    "# 6. EVALUASI\n",
    "# ============================\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(\"Akurasi BoW + Random Forest:\", accuracy)\n",
    "print(\"=======================================\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
